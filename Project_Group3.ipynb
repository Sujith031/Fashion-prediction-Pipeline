{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, regexp_extract\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GenderDetectionPipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "@udf(returnType=VectorUDT())\n",
    "def preprocess_image(content):\n",
    "    img = cv2.imdecode(np.frombuffer(content, np.uint8), cv2.IMREAD_COLOR)\n",
    "    resized = cv2.resize(img, (128, 128))\n",
    "    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "    return Vectors.dense(gray.flatten().tolist())\n",
    "\n",
    "images_df = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(\"dataset_small/{female,male}/*.jpg\")\n",
    "\n",
    "preprocessed_df = images_df.withColumn(\"features\", preprocess_image(\"content\"))\n",
    "preprocessed_df = preprocessed_df.withColumn(\"gender\", regexp_extract(\"path\", \"dataset_small/([^/]+)\", 1))\n",
    "\n",
    "\n",
    "# Save preprocessed images to a paraquet file\n",
    "preprocessed_df.write.parquet(\"Preprocessed\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'gender_detection_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Gender Detection Pipeline',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "def check_folder_exists(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise ValueError(f\"Folder {folder_path} does not exist\")\n",
    "\n",
    "check_folder_task = PythonOperator(\n",
    "    task_id='check_folder_exists',\n",
    "    python_callable=check_folder_exists,\n",
    "    op_kwargs={'folder_path': 'include/dataset/{female,male}'},\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "preprocess_images_task = SparkSubmitOperator(\n",
    "    task_id='preprocess_images',\n",
    "    application='preprocess_images.py',\n",
    "    conn_id='spark_default',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "check_folder_task >> preprocess_images_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/16 09:56:40 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Gary\\AppData\\Local\\Temp\\tmpswmkdqtn\\model, flavor: spark). Fall back to return ['pyspark==3.5.1']. Set logging level to DEBUG to see the full traceback. \n",
      "Registered model 'model_64_4_layers' already exists. Creating a new version of this model...\n",
      "2024/05/16 09:56:40 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: model_64_4_layers, version 2\n",
      "Created version '2' of model 'model_64_4_layers'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID for model_64_4_layers: 981b2e9f643f4511bc58d1372897dd27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/16 09:57:03 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Gary\\AppData\\Local\\Temp\\tmpb0md9ipt\\model, flavor: spark). Fall back to return ['pyspark==3.5.1']. Set logging level to DEBUG to see the full traceback. \n",
      "Registered model 'model_32_4_layers' already exists. Creating a new version of this model...\n",
      "2024/05/16 09:57:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: model_32_4_layers, version 2\n",
      "Created version '2' of model 'model_32_4_layers'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID for model_32_4_layers: fd853578b93344beacbe6f053d4bd0d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/16 09:57:28 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Gary\\AppData\\Local\\Temp\\tmplytzdj4e\\model, flavor: spark). Fall back to return ['pyspark==3.5.1']. Set logging level to DEBUG to see the full traceback. \n",
      "Registered model 'model_32_5_layers' already exists. Creating a new version of this model...\n",
      "2024/05/16 09:57:29 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: model_32_5_layers, version 2\n",
      "Created version '2' of model 'model_32_5_layers'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID for model_32_5_layers: 115c37b3fe6248b4b17b895eaeb076a3\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import mlflow\n",
    "import mlflow.pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CNNExperiment\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the preprocessed Parquet data\n",
    "preprocessed_data = spark.read.parquet(\"Preprocessed\")\n",
    "\n",
    "# Adding Label\n",
    "preprocessed_df = preprocessed_data.withColumn(\"label\",\n",
    "                                        when(preprocessed_data.gender == \"female\", 0)\n",
    "                                        .when(preprocessed_data.gender == \"male\", 1)\n",
    "                                        .otherwise(None))\n",
    "preprocessed_df = preprocessed_df.select(\"features\", \"label\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_data, test_data) = preprocessed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "# Set up MLflow tracking\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Replace with your MLflow tracking server URI\n",
    "mlflow.set_experiment(\"CNN_Experiment\")\n",
    "\n",
    "# Define the layer configurations to try\n",
    "layer_configs = [\n",
    "    [preprocessed_df.select(\"features\").first()[0].size, 64, 32, 2],\n",
    "    [preprocessed_df.select(\"features\").first()[0].size, 32, 16, 2],\n",
    "    [preprocessed_df.select(\"features\").first()[0].size, 32, 32, 32, 2]\n",
    "]\n",
    "\n",
    "# Iterate over activation functions and layer configurations\n",
    "for layers in layer_configs:\n",
    "        # Create the CNN model\n",
    "        stepSize = (random.random()+0.01)*0.1\n",
    "        cnn = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, solver=\"gd\", stepSize=stepSize)\n",
    "        \n",
    "        # Train the model\n",
    "        with mlflow.start_run() as run:\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"layers\", layers)\n",
    "            mlflow.log_param(\"maxIter\", 100)\n",
    "            mlflow.log_param(\"Step_size\",stepSize)\n",
    "            \n",
    "            # Train the model\n",
    "            model = cnn.fit(train_data)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            predictions = model.transform(test_data)\n",
    "            evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "            accuracy = evaluator.evaluate(predictions)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            \n",
    "            # Save the model to a file\n",
    "            model_name = f\"model_{layers[1]}_{len(layers)}_layers\"\n",
    "            model_path = f\"model/{model_name}\"\n",
    "            model.write().overwrite().save(model_path)\n",
    "            mlflow.spark.log_model(model, \"model\", registered_model_name=model_name)\n",
    "            \n",
    "            # Print the run ID for reference\n",
    "            print(f\"Run ID for {model_name}: {run.info.run_id}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gary\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.4998 - loss: 183.6264 - val_accuracy: 0.5096 - val_loss: 0.6951\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 154ms/step - accuracy: 0.6017 - loss: 0.6879 - val_accuracy: 0.6178 - val_loss: 0.6778\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.6754 - loss: 0.6100 - val_accuracy: 0.6369 - val_loss: 0.6461\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.7227 - loss: 0.5269 - val_accuracy: 0.7134 - val_loss: 0.7413\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.8137 - loss: 0.4627 - val_accuracy: 0.7261 - val_loss: 0.5616\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.8940 - loss: 0.3135 - val_accuracy: 0.7580 - val_loss: 0.6424\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 143ms/step - accuracy: 0.9055 - loss: 0.2609 - val_accuracy: 0.7197 - val_loss: 0.7223\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.8962 - loss: 0.2255 - val_accuracy: 0.7070 - val_loss: 0.6590\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 141ms/step - accuracy: 0.9156 - loss: 0.2540 - val_accuracy: 0.7898 - val_loss: 0.8099\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 0.9027 - loss: 0.2033 - val_accuracy: 0.7898 - val_loss: 0.6750\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7889 - loss: 0.8061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/17 08:46:33 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "Registered model 'cnn_model_1_[32, 64, 64]_64' already exists. Creating a new version of this model...\n",
      "2024/05/17 08:46:39 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: cnn_model_1_[32, 64, 64]_64, version 2\n",
      "Created version '2' of model 'cnn_model_1_[32, 64, 64]_64'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID for cnn_model_1_[32, 64, 64]_64: 81d5764a961e476eba9c72e1fc076b0f\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.5100 - loss: 85.1658 - val_accuracy: 0.6115 - val_loss: 0.7678\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7896 - loss: 0.4571 - val_accuracy: 0.7389 - val_loss: 0.5261\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.8881 - loss: 0.3055 - val_accuracy: 0.8344 - val_loss: 0.4668\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9421 - loss: 0.1785 - val_accuracy: 0.8280 - val_loss: 0.4853\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9533 - loss: 0.1421 - val_accuracy: 0.8471 - val_loss: 0.4637\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9757 - loss: 0.0786 - val_accuracy: 0.8408 - val_loss: 0.4873\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9814 - loss: 0.0589 - val_accuracy: 0.8599 - val_loss: 0.5189\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9998 - loss: 0.0164 - val_accuracy: 0.8408 - val_loss: 0.6709\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9986 - loss: 0.0139 - val_accuracy: 0.8471 - val_loss: 0.6175\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.9974 - loss: 0.0161 - val_accuracy: 0.8662 - val_loss: 0.6871\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8855 - loss: 0.6248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/17 08:46:57 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "Successfully registered model 'cnn_model_2_[16, 32, 64]_32'.\n",
      "2024/05/17 08:47:02 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: cnn_model_2_[16, 32, 64]_32, version 1\n",
      "Created version '1' of model 'cnn_model_2_[16, 32, 64]_32'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID for cnn_model_2_[16, 32, 64]_32: 18a3e7d2ff294b9ab06861d17a2bb14d\n",
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.5484 - loss: 9.6225 - val_accuracy: 0.5159 - val_loss: 0.6924\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.4829 - loss: 1.3847 - val_accuracy: 0.5096 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.5067 - loss: 0.6931 - val_accuracy: 0.5096 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.4946 - loss: 0.6932 - val_accuracy: 0.5096 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5002 - loss: 0.6932 - val_accuracy: 0.5096 - val_loss: 0.6931\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.4828 - loss: 0.6933 - val_accuracy: 0.5096 - val_loss: 0.6931\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5265 - loss: 0.6930 - val_accuracy: 0.5096 - val_loss: 0.6931\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4851 - loss: 0.6933 - val_accuracy: 0.5096 - val_loss: 0.6931\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.5064 - loss: 0.6932 - val_accuracy: 0.5096 - val_loss: 0.6931\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4941 - loss: 0.6932 - val_accuracy: 0.5096 - val_loss: 0.6931\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5401 - loss: 0.6928 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/17 08:47:11 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "Successfully registered model 'cnn_model_3_[8, 16, 32]_16'.\n",
      "2024/05/17 08:47:17 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: cnn_model_3_[8, 16, 32]_16, version 1\n",
      "Created version '1' of model 'cnn_model_3_[8, 16, 32]_16'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID for cnn_model_3_[8, 16, 32]_16: 486f57494ed94b6c99959ff99dc0fadb\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CNNExperiment\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the preprocessed Parquet data\n",
    "preprocessed_data = spark.read.parquet(\"Preprocessed\")\n",
    "\n",
    "# Adding Label\n",
    "preprocessed_df = preprocessed_data.withColumn(\"label\",\n",
    "                                        when(preprocessed_data.gender == \"female\", 0)\n",
    "                                        .when(preprocessed_data.gender == \"male\", 1)\n",
    "                                        .otherwise(None))\n",
    "preprocessed_df = preprocessed_df.select(\"features\", \"label\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_data, test_data) = preprocessed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Set up MLflow tracking\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Replace with your MLflow tracking server URI\n",
    "mlflow.set_experiment(\"CNN_Experiment\")\n",
    "\n",
    "# Define the CNN model architecture\n",
    "def create_cnn_model(input_shape,config):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(config['filters'][0], (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(config['filters'][1], (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(config['filters'][2], (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(config['dense_units'], activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Iterate over different CNN configurations\n",
    "cnn_configs = [\n",
    "    {'filters': [32, 64, 64], 'dense_units': 64},\n",
    "    {'filters': [16, 32, 64], 'dense_units': 32},\n",
    "    {'filters': [8, 16, 32], 'dense_units': 16}\n",
    "]\n",
    "\n",
    "# Get the input shape from the preprocessed data\n",
    "input_shape = (128, 128, 1)\n",
    "i = 1\n",
    "\n",
    "for config in cnn_configs:\n",
    "    with mlflow.start_run() as run:\n",
    "        # Create the CNN model\n",
    "        model = create_cnn_model(input_shape,config)\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"filters\", config['filters'])\n",
    "        mlflow.log_param(\"dense_units\", config['dense_units'])\n",
    "        \n",
    "        # Convert Spark DataFrame to NumPy arrays\n",
    "        train_data_np = np.array(train_data.select(\"features\").collect())\n",
    "        test_data_np = np.array(test_data.select(\"features\").collect())\n",
    "        train_data_np_lab = np.array(train_data.select(\"label\").collect())\n",
    "        test_data_np_lab = np.array(test_data.select(\"label\").collect())\n",
    "        \n",
    "        X_train = train_data_np[:, 0].reshape(-1, input_shape[0], input_shape[1], 1)\n",
    "        y_train = train_data_np_lab\n",
    "        X_test = test_data_np[:, 0].reshape(-1, input_shape[0], input_shape[1], 1)\n",
    "        y_test = test_data_np_lab\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        _, accuracy = model.evaluate(X_test, y_test)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        \n",
    "        # Save the model\n",
    "        model_name = f\"cnn_model_{i}_{config['filters']}_{config['dense_units']}\"\n",
    "        mlflow.keras.log_model(model, \"model\", registered_model_name=model_name)\n",
    "        model.save(f\"model/cnn_model_{i}.keras\")\n",
    "        i+=1\n",
    "        \n",
    "        # Print the run ID for reference\n",
    "        print(f\"Run ID for {model_name}: {run.info.run_id}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from fastapi import FastAPI, Request, File, UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import uvicorn\n",
    "from starlette.middleware.base import BaseHTTPMiddleware\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from prometheus_client import Counter, Histogram, CollectorRegistry, generate_latest\n",
    "from starlette.responses import Response\n",
    "\n",
    "REQUESTS = Counter('http_requests_total', 'Total HTTP Requests', ['method', 'endpoint', 'status_code'])\n",
    "REQUEST_LATENCY = Histogram('http_request_latency_seconds', 'HTTP Request Latency', ['method', 'endpoint'])\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the trained Keras model\n",
    "model_path = \"model/cnn_model_2.keras\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    resized = cv2.resize(image, (128, 128))\n",
    "    gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "    normalized = gray / 255.0\n",
    "    reshaped = np.reshape(normalized, (1, 128, 128, 1))\n",
    "    return reshaped\n",
    "\n",
    "class RateLimitingMiddleware(BaseHTTPMiddleware):\n",
    "    def __init__(self, app, limit=1000, duration=60):\n",
    "        super().__init__(app)\n",
    "        self.limit = limit\n",
    "        self.duration = duration\n",
    "        self.request_counts = defaultdict(lambda: (0, datetime.now()))\n",
    "\n",
    "    async def dispatch(self, request: Request, call_next):\n",
    "        client_ip = request.client.host\n",
    "        count, last_request_time = self.request_counts[client_ip]\n",
    "\n",
    "        if count >= self.limit and (datetime.now() - last_request_time).total_seconds() < self.duration:\n",
    "            return JSONResponse(\n",
    "                status_code=429,\n",
    "                content={\"error\": f\"Rate limit exceeded for IP {client_ip}. Please try again later.\"}\n",
    "            )\n",
    "\n",
    "        self.request_counts[client_ip] = (count + 1, datetime.now())\n",
    "        response = await call_next(request)\n",
    "        return response\n",
    "\n",
    "app.add_middleware(RateLimitingMiddleware)\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def prometheus_middleware(request: Request, call_next):\n",
    "    method = request.method\n",
    "    path = request.url.path\n",
    "\n",
    "    with REQUEST_LATENCY.labels(method=method, endpoint=path).time():\n",
    "        response = await call_next(request)\n",
    "\n",
    "    status_code = response.status_code\n",
    "    REQUESTS.labels(method=method, endpoint=path, status_code=status_code).inc()\n",
    "\n",
    "    return response\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "def metrics():\n",
    "    registry = CollectorRegistry()\n",
    "    registry.register(REQUESTS)\n",
    "    registry.register(REQUEST_LATENCY)\n",
    "    return Response(generate_latest(registry), media_type=\"text/plain\")\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict_gender(request: Request, file: UploadFile = File(...)):\n",
    "    # Read the uploaded image file\n",
    "    img = Image.open(file.file)\n",
    "    img = np.array(img)\n",
    "\n",
    "    # Preprocess the image\n",
    "    features = preprocess_image(img)\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    prediction = model.predict(features)\n",
    "\n",
    "    # Map the prediction to gender\n",
    "    gender = 'female' if prediction[0][0] < 0.5 else 'male'\n",
    "\n",
    "    # Get the client IP address\n",
    "    client_ip = request.client.host\n",
    "\n",
    "    return {\"gender\": gender}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3818.load.\n: java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel$MultilayerPerceptronClassificationModelReader.load(MultilayerPerceptronClassifier.scala:383)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel$MultilayerPerceptronClassificationModelReader.load(MultilayerPerceptronClassifier.scala:376)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenderDetectionAPI\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     10\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodel_32_4_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m \u001b[43mMultilayerPerceptronClassificationModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m],model_path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\util.py:318\u001b[0m, in \u001b[0;36mJavaMLReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[1;32m--> 318\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_java\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3818.load.\n: java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel$MultilayerPerceptronClassificationModelReader.load(MultilayerPerceptronClassifier.scala:383)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel$MultilayerPerceptronClassificationModelReader.load(MultilayerPerceptronClassifier.scala:376)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n",
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GenderDetectionAPI\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "model_path = \"model\\\\model_32_4_layers\"\n",
    "model_2 = MultilayerPerceptronClassificationModel.load(model_path)\n",
    "os.path.join(sys.argv[1],model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
